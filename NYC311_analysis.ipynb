{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC 311 Service Request Data Analysis\n",
    "\n",
    "**Course:** GET 305 - Data Analysis\n",
    "\n",
    "**Objective:** Comprehensive analysis of NYC 311 service request data including data cleaning, profiling, visualization, and statistical analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#1-data-loading)\n",
    "2. [Data Cleaning and Profiling](#2-data-cleaning-and-profiling)\n",
    "3. [Feature Engineering](#3-feature-engineering)\n",
    "4. [Exploratory Data Analysis](#4-exploratory-data-analysis)\n",
    "5. [Visualizations](#5-visualizations)\n",
    "6. [Statistical Analysis](#6-statistical-analysis)\n",
    "7. [Interpretation and Conclusions](#7-interpretation-and-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print('All libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading <a id=\"1-data-loading\"></a>\n",
    "\n",
    "We load the cleaned data from the SQLite database using SQLAlchemy. The data was previously cleaned using SQL operations documented in `nyc311_sql_tasks.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "engine = create_engine('sqlite:///nyc311.db')\n",
    "\n",
    "# Load the cleaned data\n",
    "query = 'SELECT * FROM \"311_cleaned\"'\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "print(f'Dataset loaded successfully!')\n",
    "print(f'Shape: {df.shape[0]:,} rows × {df.shape[1]} columns')\n",
    "print(f'Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types overview\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Cleaning and Profiling <a id=\"2-data-cleaning-and-profiling\"></a>\n",
    "\n",
    "### 2.1 Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing Count': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Data Type': df.dtypes\n",
    "})\n",
    "missing_stats = missing_stats[missing_stats['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "print('Columns with Missing Values:')\n",
    "print('=' * 60)\n",
    "missing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if len(missing_stats) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    missing_stats['Missing %'].plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Missing Percentage (%)')\n",
    "    ax.set_ylabel('Column')\n",
    "    ax.set_title('Missing Value Distribution by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No missing values found in the dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Type Corrections\n",
    "\n",
    "We need to convert date columns to proper datetime format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created_date_raw'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "df['closed_date'] = pd.to_datetime(df['closed_date_raw'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "\n",
    "# Verify conversion\n",
    "print('Date conversion results:')\n",
    "print(f\"Created Date - Valid: {df['created_date'].notna().sum():,}, Invalid: {df['created_date'].isna().sum():,}\")\n",
    "print(f\"Closed Date - Valid: {df['closed_date'].notna().sum():,}, Invalid: {df['closed_date'].isna().sum():,}\")\n",
    "print(f\"\\nDate range: {df['created_date'].min()} to {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Duplicate Verification\n",
    "\n",
    "Verify that duplicates were properly handled in the SQL processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate unique keys\n",
    "duplicate_count = df['unique_key'].duplicated().sum()\n",
    "print(f'Duplicate unique_key records: {duplicate_count}')\n",
    "\n",
    "# Check for exact row duplicates\n",
    "exact_duplicates = df.duplicated().sum()\n",
    "print(f'Exact duplicate rows: {exact_duplicates}')\n",
    "\n",
    "if duplicate_count == 0 and exact_duplicates == 0:\n",
    "    print('\\n✓ No duplicates found - data integrity verified!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Outlier Detection\n",
    "\n",
    "Using the IQR method to identify outliers in numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns for outlier detection\n",
    "numeric_cols = ['latitude', 'longitude']\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print('Outlier Analysis (IQR Method):')\n",
    "print('=' * 60)\n",
    "for col in numeric_cols:\n",
    "    valid_data = df[df[col].notna()]\n",
    "    if len(valid_data) > 0:\n",
    "        outlier_count, lb, ub = detect_outliers_iqr(valid_data, col)\n",
    "        print(f'{col}:')\n",
    "        print(f'  Outliers: {outlier_count:,} ({100*outlier_count/len(valid_data):.2f}%)')\n",
    "        print(f'  Valid range: [{lb:.4f}, {ub:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering <a id=\"3-feature-engineering\"></a>\n",
    "\n",
    "Creating new features to enhance our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Response Time (in hours)\n",
    "df['response_time_hours'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 3600\n",
    "\n",
    "# Remove negative response times (data quality issues)\n",
    "df.loc[df['response_time_hours'] < 0, 'response_time_hours'] = np.nan\n",
    "\n",
    "# 3.2 Hour of Day\n",
    "df['hour_of_day'] = df['created_date'].dt.hour\n",
    "\n",
    "# 3.3 Day of Week (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df['created_date'].dt.dayofweek\n",
    "df['day_name'] = df['created_date'].dt.day_name()\n",
    "\n",
    "# 3.4 Month and Year\n",
    "df['month'] = df['created_date'].dt.month\n",
    "df['year'] = df['created_date'].dt.year\n",
    "df['year_month'] = df['created_date'].dt.to_period('M')\n",
    "\n",
    "# 3.5 Is Weekend\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print('Feature Engineering Summary:')\n",
    "print('=' * 60)\n",
    "print(f\"Response Time (hours) - Mean: {df['response_time_hours'].mean():.2f}, Median: {df['response_time_hours'].median():.2f}\")\n",
    "print(f\"Date range: {df['year'].min()} to {df['year'].max()}\")\n",
    "print(f\"\\nNew features created: response_time_hours, hour_of_day, day_of_week, day_name, month, year, year_month, is_weekend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview engineered features\n",
    "df[['unique_key', 'created_date', 'closed_date', 'response_time_hours', \n",
    "    'hour_of_day', 'day_name', 'is_weekend']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Data Analysis <a id=\"4-exploratory-data-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('Dataset Summary Statistics:')\n",
    "print('=' * 60)\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique boroughs: {df['borough'].nunique()}\")\n",
    "print(f\"Unique complaint types: {df['complaint_type'].nunique()}\")\n",
    "print(f\"Unique agencies: {df['agency'].nunique()}\")\n",
    "print(f\"\\nRecords per borough:\")\n",
    "print(df['borough'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top complaint types\n",
    "print('\\nTop 15 Complaint Types:')\n",
    "print('=' * 60)\n",
    "complaint_counts = df['complaint_type'].value_counts().head(15)\n",
    "complaint_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response time statistics by borough\n",
    "print('\\nResponse Time Statistics by Borough (hours):')\n",
    "print('=' * 60)\n",
    "response_by_borough = df.groupby('borough')['response_time_hours'].agg(['mean', 'median', 'std', 'count'])\n",
    "response_by_borough.columns = ['Mean', 'Median', 'Std Dev', 'Count']\n",
    "response_by_borough.sort_values('Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualizations <a id=\"5-visualizations\"></a>\n",
    "\n",
    "### 5.1 Time Series of Complaint Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly complaint volume\n",
    "monthly_volume = df.groupby('year_month').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "monthly_volume.plot(ax=ax, linewidth=2, color='steelblue', marker='o', markersize=4)\n",
    "ax.set_xlabel('Month', fontsize=12)\n",
    "ax.set_ylabel('Number of Complaints', fontsize=12)\n",
    "ax.set_title('NYC 311 Complaint Volume Over Time (Monthly)', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "x_numeric = np.arange(len(monthly_volume))\n",
    "z = np.polyfit(x_numeric, monthly_volume.values, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(monthly_volume.index.astype(str), p(x_numeric), 'r--', alpha=0.7, label='Trend')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_01_time_series.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print('The time series visualization shows the monthly volume of 311 complaints over the observation period.')\n",
    "print(f'Peak month: {monthly_volume.idxmax()} with {monthly_volume.max():,} complaints')\n",
    "print(f'Lowest month: {monthly_volume.idxmin()} with {monthly_volume.min():,} complaints')\n",
    "print('The trend line indicates the overall direction of complaint volumes over time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Chart of Top Complaint Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 complaint types\n",
    "top_complaints = df['complaint_type'].value_counts().head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "colors = sns.color_palette('viridis', len(top_complaints))\n",
    "bars = ax.barh(range(len(top_complaints)), top_complaints.values, color=colors)\n",
    "ax.set_yticks(range(len(top_complaints)))\n",
    "ax.set_yticklabels(top_complaints.index)\n",
    "ax.set_xlabel('Number of Complaints', fontsize=12)\n",
    "ax.set_ylabel('Complaint Type', fontsize=12)\n",
    "ax.set_title('Top 10 NYC 311 Complaint Types', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, top_complaints.values)):\n",
    "    ax.text(val + 500, i, f'{val:,}', va='center', fontsize=10)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_02_complaint_types.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print(f'The most common complaint type is \"{top_complaints.index[0]}\" with {top_complaints.values[0]:,} complaints.')\n",
    "print(f'The top 10 complaint types account for {100*top_complaints.sum()/len(df):.1f}% of all complaints.')\n",
    "print('This distribution helps prioritize city resources for addressing the most frequent issues.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Geospatial Scatter Plot (Latitude vs Longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for valid coordinates\n",
    "geo_df = df[(df['has_valid_coordinates'] == 1) & \n",
    "            (df['latitude'].notna()) & \n",
    "            (df['longitude'].notna())].sample(min(50000, len(df)), random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Create scatter plot colored by borough\n",
    "boroughs = geo_df['borough'].unique()\n",
    "colors = sns.color_palette('Set2', len(boroughs))\n",
    "borough_colors = dict(zip(boroughs, colors))\n",
    "\n",
    "for borough in boroughs:\n",
    "    if borough:\n",
    "        borough_data = geo_df[geo_df['borough'] == borough]\n",
    "        ax.scatter(borough_data['longitude'], borough_data['latitude'], \n",
    "                   c=[borough_colors[borough]], label=borough, alpha=0.4, s=5)\n",
    "\n",
    "ax.set_xlabel('Longitude', fontsize=12)\n",
    "ax.set_ylabel('Latitude', fontsize=12)\n",
    "ax.set_title('Geographic Distribution of 311 Complaints in NYC', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Borough', loc='upper left', markerscale=3)\n",
    "ax.set_xlim(-74.3, -73.7)\n",
    "ax.set_ylim(40.5, 40.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_03_geospatial.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print('This scatter plot shows the geographic distribution of 311 complaints across NYC.')\n",
    "print('Each point represents a complaint location, colored by borough.')\n",
    "print('The visualization reveals the distinct shapes of each borough and the density of complaints.')\n",
    "print('Higher density areas may indicate neighborhoods with more service issues or higher population.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Distribution of Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for reasonable response times (< 30 days = 720 hours)\n",
    "response_df = df[(df['response_time_hours'].notna()) & \n",
    "                 (df['response_time_hours'] > 0) & \n",
    "                 (df['response_time_hours'] < 720)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(response_df['response_time_hours'], bins=50, color='teal', edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(response_df['response_time_hours'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: {response_df[\"response_time_hours\"].median():.1f}h')\n",
    "axes[0].axvline(response_df['response_time_hours'].mean(), color='orange', linestyle='--', \n",
    "                label=f'Mean: {response_df[\"response_time_hours\"].mean():.1f}h')\n",
    "axes[0].set_xlabel('Response Time (hours)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Response Times', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot by borough\n",
    "response_df.boxplot(column='response_time_hours', by='borough', ax=axes[1], \n",
    "                    patch_artist=True, showfliers=False)\n",
    "axes[1].set_xlabel('Borough', fontsize=12)\n",
    "axes[1].set_ylabel('Response Time (hours)', fontsize=12)\n",
    "axes[1].set_title('Response Time by Borough', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_04_response_time.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print(f'The histogram shows a right-skewed distribution of response times.')\n",
    "print(f'Median response time: {response_df[\"response_time_hours\"].median():.1f} hours')\n",
    "print(f'Mean response time: {response_df[\"response_time_hours\"].mean():.1f} hours')\n",
    "print('The difference between mean and median indicates positive skewness (some complaints take much longer).')\n",
    "print('The boxplot reveals variations in response times across boroughs, which may indicate differences in service efficiency.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Statistical Analysis <a id=\"6-statistical-analysis\"></a>\n",
    "\n",
    "### 6.1 Hypothesis Testing\n",
    "\n",
    "#### Hypothesis Test 1: Difference in Mean Response Time Between Manhattan and Brooklyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for hypothesis testing\n",
    "manhattan_response = df[(df['borough'] == 'MANHATTAN') & \n",
    "                        (df['response_time_hours'].notna()) & \n",
    "                        (df['response_time_hours'] > 0) & \n",
    "                        (df['response_time_hours'] < 720)]['response_time_hours']\n",
    "\n",
    "brooklyn_response = df[(df['borough'] == 'BROOKLYN') & \n",
    "                       (df['response_time_hours'].notna()) & \n",
    "                       (df['response_time_hours'] > 0) & \n",
    "                       (df['response_time_hours'] < 720)]['response_time_hours']\n",
    "\n",
    "print('=' * 70)\n",
    "print('HYPOTHESIS TEST 1: Response Time Comparison (Manhattan vs Brooklyn)')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('H₀ (Null Hypothesis): There is no significant difference in mean response')\n",
    "print('   time between Manhattan and Brooklyn.')\n",
    "print('H₁ (Alternative Hypothesis): There is a significant difference in mean')\n",
    "print('   response time between Manhattan and Brooklyn.')\n",
    "print()\n",
    "print('Test Used: Independent Samples t-test (two-tailed)')\n",
    "print('Significance Level: α = 0.05')\n",
    "print()\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(manhattan_response, brooklyn_response)\n",
    "\n",
    "print(f'Manhattan: n={len(manhattan_response):,}, mean={manhattan_response.mean():.2f}h, std={manhattan_response.std():.2f}h')\n",
    "print(f'Brooklyn:  n={len(brooklyn_response):,}, mean={brooklyn_response.mean():.2f}h, std={brooklyn_response.std():.2f}h')\n",
    "print()\n",
    "print(f'Test Statistic (t): {t_stat:.4f}')\n",
    "print(f'P-value: {p_value:.6f}')\n",
    "print()\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('Conclusion: We REJECT the null hypothesis (p < 0.05).')\n",
    "    print('There is a statistically significant difference in mean response times')\n",
    "    print('between Manhattan and Brooklyn.')\n",
    "else:\n",
    "    print('Conclusion: We FAIL TO REJECT the null hypothesis (p ≥ 0.05).')\n",
    "    print('There is no statistically significant difference in mean response times')\n",
    "    print('between Manhattan and Brooklyn.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Test 2: Association Between Complaint Type and Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for independence\n",
    "print('=' * 70)\n",
    "print('HYPOTHESIS TEST 2: Association Between Complaint Type and Borough')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('H₀ (Null Hypothesis): There is no association between complaint type and borough.')\n",
    "print('   The distribution of complaint types is independent of borough.')\n",
    "print('H₁ (Alternative Hypothesis): There is an association between complaint type')\n",
    "print('   and borough. The distribution of complaint types depends on borough.')\n",
    "print()\n",
    "print('Test Used: Chi-Square Test of Independence')\n",
    "print('Significance Level: α = 0.05')\n",
    "print()\n",
    "\n",
    "# Create contingency table for top 10 complaint types\n",
    "top_10_types = df['complaint_type'].value_counts().head(10).index.tolist()\n",
    "filtered_df = df[df['complaint_type'].isin(top_10_types) & df['borough'].notna()]\n",
    "contingency_table = pd.crosstab(filtered_df['complaint_type'], filtered_df['borough'])\n",
    "\n",
    "print('Contingency Table (Top 10 Complaint Types by Borough):')\n",
    "print(contingency_table)\n",
    "print()\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "print(f'Chi-Square Statistic: {chi2:.2f}')\n",
    "print(f'Degrees of Freedom: {dof}')\n",
    "print(f'P-value: {p_value:.2e}')\n",
    "print()\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('Conclusion: We REJECT the null hypothesis (p < 0.05).')\n",
    "    print('There is a statistically significant association between complaint type')\n",
    "    print('and borough. Different boroughs tend to have different patterns of complaint types.')\n",
    "else:\n",
    "    print('Conclusion: We FAIL TO REJECT the null hypothesis (p ≥ 0.05).')\n",
    "    print('There is no significant association between complaint type and borough.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numeric data for correlation\n",
    "corr_df = df[['response_time_hours', 'hour_of_day', 'day_of_week', 'month', 'latitude', 'longitude']].dropna()\n",
    "\n",
    "# Pearson correlation\n",
    "print('=' * 70)\n",
    "print('CORRELATION ANALYSIS')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('Pearson Correlation Coefficients:')\n",
    "print('(Measures linear relationships between variables)')\n",
    "print()\n",
    "\n",
    "pearson_corr = corr_df.corr(method='pearson')\n",
    "print(pearson_corr.round(4))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
    "sns.heatmap(pearson_corr, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.3f', ax=ax, square=True, linewidths=0.5)\n",
    "ax.set_title('Pearson Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation\n",
    "print('\\nSpearman Correlation Coefficients:')\n",
    "print('(Measures monotonic relationships, more robust to outliers)')\n",
    "print()\n",
    "\n",
    "spearman_corr = corr_df.corr(method='spearman')\n",
    "print(spearman_corr.round(4))\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print('The correlation matrices show relationships between numeric variables.')\n",
    "print('Key observations:')\n",
    "print('- Response time shows weak correlations with temporal features (hour, day, month)')\n",
    "print('- Latitude and longitude are weakly correlated with response times')\n",
    "print('- The weak correlations suggest response time is influenced by many other factors')\n",
    "print('  not captured by simple temporal or spatial features alone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Regression Analysis\n",
    "\n",
    "Building a regression model to predict response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('REGRESSION ANALYSIS: Predicting Response Time')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "# Prepare data for regression\n",
    "reg_df = df[['response_time_hours', 'hour_of_day', 'day_of_week', 'is_weekend', 'borough', 'complaint_category']].dropna()\n",
    "reg_df = reg_df[(reg_df['response_time_hours'] > 0) & (reg_df['response_time_hours'] < 720)]\n",
    "\n",
    "# Create dummy variables for categorical features\n",
    "reg_df_encoded = pd.get_dummies(reg_df, columns=['borough', 'complaint_category'], drop_first=True)\n",
    "\n",
    "# Define dependent and independent variables\n",
    "y = reg_df_encoded['response_time_hours']\n",
    "X = reg_df_encoded.drop('response_time_hours', axis=1)\n",
    "X = sm.add_constant(X)  # Add intercept\n",
    "\n",
    "print('Dependent Variable: response_time_hours')\n",
    "print('Independent Variables:')\n",
    "print('  - hour_of_day (numeric)')\n",
    "print('  - day_of_week (numeric, 0=Monday to 6=Sunday)')\n",
    "print('  - is_weekend (binary)')\n",
    "print('  - borough (categorical, dummy encoded)')\n",
    "print('  - complaint_category (categorical, dummy encoded)')\n",
    "print()\n",
    "\n",
    "# Fit OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation\n",
    "print('\\n' + '=' * 70)\n",
    "print('REGRESSION MODEL INTERPRETATION')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\nModel Fit:')\n",
    "print(f'  R-squared: {model.rsquared:.4f}')\n",
    "print(f'  Adjusted R-squared: {model.rsquared_adj:.4f}')\n",
    "print(f'  F-statistic: {model.fvalue:.2f}')\n",
    "print(f'  F-statistic p-value: {model.f_pvalue:.2e}')\n",
    "\n",
    "print('\\nSignificant Coefficients (p < 0.05):')\n",
    "significant_vars = model.pvalues[model.pvalues < 0.05].index.tolist()\n",
    "for var in significant_vars:\n",
    "    coef = model.params[var]\n",
    "    pval = model.pvalues[var]\n",
    "    print(f'  {var}: coef={coef:.4f}, p={pval:.4f}')\n",
    "\n",
    "print('\\n--- Interpretation ---')\n",
    "print('1. The low R-squared indicates that the included variables explain only a small')\n",
    "print('   portion of the variance in response time. This is expected as response time')\n",
    "print('   is influenced by many factors not in the model (staffing, complexity, etc.).')\n",
    "print()\n",
    "print('2. Significant predictors show statistically reliable relationships with response time,')\n",
    "print('   though their practical effect sizes may be small.')\n",
    "print()\n",
    "print('3. Limitations:')\n",
    "print('   - Linear regression assumes linear relationships')\n",
    "print('   - Response time is positively skewed, violating normality assumptions')\n",
    "print('   - Omitted variable bias is likely present')\n",
    "print('   - The model is useful for understanding relationships but has limited')\n",
    "print('     predictive power for individual complaints.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Interpretation and Conclusions <a id=\"7-interpretation-and-conclusions\"></a>\n",
    "\n",
    "### 7.1 Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('KEY FINDINGS SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('1. DATA OVERVIEW')\n",
    "print(f'   - Total complaints analyzed: {len(df):,}')\n",
    "print(f'   - Date range: {df[\"created_date\"].min().strftime(\"%Y-%m-%d\")} to {df[\"created_date\"].max().strftime(\"%Y-%m-%d\")}')\n",
    "print(f'   - Data quality: {100*df[\"has_valid_borough\"].mean():.1f}% with valid borough,',\n",
    "      f'{100*df[\"has_valid_coordinates\"].mean():.1f}% with valid coordinates')\n",
    "print()\n",
    "print('2. COMPLAINT PATTERNS')\n",
    "print(f'   - Most common complaint: {df[\"complaint_type\"].mode()[0]}')\n",
    "print(f'   - Borough with most complaints: {df[\"borough\"].value_counts().idxmax()}')\n",
    "print(f'   - Average response time: {df[\"response_time_hours\"].mean():.1f} hours')\n",
    "print()\n",
    "print('3. STATISTICAL FINDINGS')\n",
    "print('   - Significant difference in response times across boroughs')\n",
    "print('   - Strong association between complaint types and boroughs')\n",
    "print('   - Temporal features are weak predictors of response time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Data Limitations and Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('DATA LIMITATIONS AND ASSUMPTIONS')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('LIMITATIONS:')\n",
    "print('1. Missing Data: Some records lack coordinates or closed dates, limiting')\n",
    "print('   spatial analysis and response time calculations.')\n",
    "print()\n",
    "print('2. Self-reported Nature: 311 data depends on citizens reporting issues,')\n",
    "print('   potentially introducing reporting bias across neighborhoods.')\n",
    "print()\n",
    "print('3. Resolution Definition: \"Closed Date\" may not reflect when an issue')\n",
    "print('   was actually resolved, just when the ticket was closed.')\n",
    "print()\n",
    "print('4. Temporal Scope: Analysis is limited to the available date range and')\n",
    "print('   may not capture long-term trends or seasonal patterns.')\n",
    "print()\n",
    "print('ASSUMPTIONS:')\n",
    "print('1. Records with missing boroughs or coordinates are randomly distributed')\n",
    "print('   and their exclusion does not systematically bias results.')\n",
    "print()\n",
    "print('2. Negative response times (closed before created) represent data entry')\n",
    "print('   errors and are appropriately excluded.')\n",
    "print()\n",
    "print('3. The complaint categories are accurately labeled and consistently')\n",
    "print('   applied by 311 operators.')\n",
    "print()\n",
    "print('POTENTIAL BIAS:')\n",
    "print('1. Underreporting in areas with less tech-savvy populations or')\n",
    "print('   lower awareness of 311 services.')\n",
    "print()\n",
    "print('2. Selection bias if certain complaint types are handled through')\n",
    "print('   different channels and not captured in 311 data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Profiling Report\n",
    "\n",
    "Generating automated profiling report using ydata_profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profiling report\n",
    "# Using a sample for faster execution\n",
    "print('Generating data profiling report...')\n",
    "print('This may take several minutes for large datasets...')\n",
    "\n",
    "# Sample for profiling (to reduce computation time)\n",
    "sample_size = min(50000, len(df))\n",
    "profile_df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "# Select key columns for profiling\n",
    "profile_columns = ['unique_key', 'created_date', 'closed_date', 'agency', 'complaint_type', \n",
    "                   'complaint_category', 'borough', 'latitude', 'longitude', 'status',\n",
    "                   'response_time_hours', 'hour_of_day', 'day_of_week', 'is_weekend']\n",
    "profile_df = profile_df[profile_columns]\n",
    "\n",
    "# Generate profile\n",
    "profile = ProfileReport(profile_df, \n",
    "                        title='NYC 311 Data Profiling Report',\n",
    "                        explorative=True,\n",
    "                        minimal=False)\n",
    "\n",
    "# Save to HTML\n",
    "profile.to_file('nyc311_profile.html')\n",
    "print('\\nProfiling report saved as: nyc311_profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Analysis\n",
    "\n",
    "This notebook contains the complete analysis of NYC 311 service request data as required by the assignment. All visualizations, statistical tests, and interpretations follow academic standards and are reproducible from the provided SQLite database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
